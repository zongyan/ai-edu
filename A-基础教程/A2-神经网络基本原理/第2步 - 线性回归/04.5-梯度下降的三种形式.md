<!--Copyright © Microsoft Corporation. All rights reserved.
适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 4.5 梯度下降的三种形式

我们比较一下目前我们用三种方法得到的 $w$ 和 $b$ 的值，见表4-2。

表4-2 三种方法的结果比较

|方法|$w$|$b$|
|----|----|----|
|最小二乘法|2.056827|2.965434|
|梯度下降法|1.71629006|3.19684087|
|神经网络法|1.71629006|3.19684087|

<font color="green">最小二乘法采用的就是对loss函数求导，然后令导数为零就是可以（求解便是可以得到 $w$ 和 $b$的两个数值），梯度下降和神经网络的基本思路是一样的，但是就是在思维上面是不一样的，前者就是简单的利用了迭代的方式，后者就是需要定义神经元等等的结构。</font>

这个问题的原始值是可能是 $w=2,b=3$，由于样本噪音的存在，使用最小二乘法得到了 $2.05,2.96$ 这样的非整数解，这是完全可以接受的。但是使用梯度下降和神经网络两种方式，都得到 $1.71,3.19$ 这样的值，准确程度很低。从图4-6的神经网络的训练结果来看，拟合直线是斜着穿过样本点区域的，并没有在正中央的骨架上。

<img src="../Images/4/result.png" ch="500" />

图4-6 拟合效果

难道是神经网络方法有什么问题吗？

初次使用神经网络，一定有水土不服的地方。最小二乘法可以得到数学解析解，所以它的结果是可信的。梯度下降法和神经网络法实际是一回事儿，只是梯度下降没有使用神经元模型而已。所以，接下来我们研究一下如何调整神经网络的训练过程，先从最简单的梯度下降的三种形式说起。<font color="green">原来数学解析解，是最精准的。这里也是提到了，就是这个梯度下降和神经网络本质上是一回事儿。</font>

在下面的说明中，我们使用如下假设，以便简化问题易于理解：

1. 使用可以解决本章的问题的线性回归模型，即 $z=x \cdot w+b$；
2. 样本特征值数量为1，即 $x,w,b$ 都是标量；<font color="green">注意区分样本特征值 & 样本数量的区别，可以参见 4.0 的$X$来理解（行数代表样本数量，列数代表这个每一个样本对应的特征值的数量）</font>
3. 使用均方差损失函数。

计算 $w$ 的梯度：

$$
\frac{\partial{loss}}{\partial{w}} = \frac{\partial{loss}}{\partial{z_i}}\frac{\partial{z_i}}{\partial{w}}=(z_i-y_i)x_i
$$

计算 $b$ 的梯度：

$$
\frac{\partial{loss}}{\partial{b}} = \frac{\partial{loss}}{\partial{z_i}}\frac{\partial{z_i}}{\partial{b}}=z_i-y_i
$$

### 4.5.1 单样本随机梯度下降

SGD(Stochastic Gradient Descent)

样本访问示意图如图4-7所示。
  
<img src="../Images/4/SingleSample-example.png" />

图4-7 单样本访问方式

#### 计算过程

假设一共100个样本，每次使用1个样本：

***
$repeat \lbrace \\\\
  \quad for \quad i=1,2,3,...,100  \lbrace \\\\
  \quad \quad z_i = x_i \cdot w + b\\\\
  \quad \quad dw= x_i \cdot (z_i - y_i)\\\\
  \quad \quad db= z_i - y_i \\\\
  \quad \quad w=w-\eta \cdot dw \\\\
  \quad \quad b=b-\eta \cdot db \\\\
  \quad  \rbrace  \\\\
\rbrace$

***

#### 特点
  
  - 训练样本：每次使用一个样本数据进行一次训练，更新一次梯度，重复以上过程。
  - 优点：训练开始时损失值下降很快，随机性大，找到最优解的可能性大。
  - 缺点：受单个样本的影响最大，损失函数值波动大，到后期徘徊不前，在最优解附近震荡。不能并行计算。

#### 运行结果

设置`batch_size=1`，即单样本方式：<font color="green">后面也是会提到，Epoch就是所有样本被使用一次</font>

```Python
if __name__ == '__main__':
    sdr = SimpleDataReader()
    sdr.ReadData()
    params = HyperParameters(1, 1, eta=0.1, max_epoch=100, batch_size=1, eps = 0.02)
    net = NeuralNet(params)
    net.train(sdr)
```    

表4-3 单样本方式的训练情况

|损失函数值|梯度下降过程|
|---|---|
|<img src="../Images/4/SingleSample-Loss.png"/>|<img src="../Images/4/SingleSample-Trace.png"/>|

表4-3的左图，由于我们使用了限定的停止条件，即当损失函数值小于等于 $0.02$ 时停止训练，所以，单样本方式迭代了300次后达到了精度要求。

右图是 $w$ 和 $b$ 共同构成的损失函数等高线图。梯度下降时，开始收敛较快，稍微有些弯曲地向中央地带靠近。到后期波动较大，找不到准确的前进方向，曲折地达到中心附近。

### 4.5.2 小批量样本梯度下降

Mini-Batch Gradient Descent

样本访问示意图如图4-8所示。

<img src="../Images/4/MiniBatch-example.png" />

图4-8 小批量样本访问方式

#### 计算过程

假设一共100个样本，每个小批量5个样本：<font color="green">注意看 $dw$ 和 $db$的计算方式，就是可以明白mini-Batch Gradiet Descent和Stochastic Gradient Descent的区别了。</font>

***
$repeat \lbrace \\\\
  \quad for \quad i=1,6,11,...,96 \lbrace\\\\
  \quad \quad z_i = x_i \cdot w + b \\\\ 
  \quad \quad z_{i+1} = x_{i+1} \cdot w + b \\\\ 
  \quad \quad \dots \\\\ 
  \quad \quad z_{i+4} = x_{i+4} \cdot w + b \\\\ 
  \quad \quad dw= {1 \over 5}\sum_{k=i}^{i+4} x_k \cdot (z_k - y_k) \\\\ 
  \quad \quad db= {1 \over 5}\sum_{k=i}^{i+4} (z_k - y_k) \\\\ 
  \quad \quad w=w-\eta \cdot dw \\\\ 
  \quad \quad b=b-\eta \cdot db \\\\ 
  \quad  \rbrace  \\\\
\rbrace$
***

上述算法中，循环体中的前5行分别计算了 $z_i, z_{i+1}, ..., z_{i+4}$，可以换成一次性的矩阵运算。

#### 特点

  - 训练样本：选择一小部分样本进行训练，更新一次梯度，然后再选取另外一小部分样本进行训练，再更新一次梯度。
  - 优点：不受单样本噪声影响，训练速度较快。
  - 缺点：batch size的数值选择很关键，会影响训练结果。<font color="red">ToDo: 这个就是需要查看一下文献，或者问一下他们，这个就是有没有现成的文献，或者是已有的研究，来深入的研究一下这个问题的了。</font>


#### 运行结果

设置`batch_size=10`：<font color="red">这个epoch的含义还是不太理解，就是需要再研究一下的了 --> 我终于是理解这个epoch的含义了，在Stochastic Gradient Descent中的数值是100，Mini-Batch Gradient Descent的数值是100，在Full Batch Gradient Descent中设定的数值是1000，但是以上的这一些数值，仅仅是设置的数值。但是在实际的训练过程中，训练结束的标准是模型达到了想要的训练精度（eps的数值，或者是loss function $J$小于eps的数值），此时的epoch的数据，就是总共使用了（所有数据）的次数了，如表4-6所示（比如，full batch gradient descent的数值，总是最大的，因为他一次训练，就是需要所有的数据，然后就是训练了60次（的所有数据），才能够达到想到的性能精度）</font>

```Python
if __name__ == '__main__':
    sdr = SimpleDataReader()
    sdr.ReadData()
    params = HyperParameters(1, 1, eta=0.3, max_epoch=100, batch_size=10, eps = 0.02)
    net = NeuralNet(params)
    net.train(sdr)   
```

表4-4 小批量样本方式的训练情况

|损失函数值|梯度下降过程|
|---|---|
|<img src="../Images/4/MiniBatch-Loss.png"/>|<img src="../Images/4/MiniBatch-Trace.png"/>|

表4-4的右图，梯度下降时，在接近中心时有小波动。图太小看不清楚，可以用matplot工具放大局部来观察。和单样本方式比较，在中心区的波动已经缓解了很多。

小批量的大小通常由以下几个因素决定：<font color="green">我还是不太明白这个下批量batch size的选择规则，需要更多的阅读</font>

- 更大的批量会计算更精确的梯度，但是回报却是小于线性的。
- 极小批量通常难以充分利用多核架构。这决定了最小批量的数值，低于这个值的小批量处理不会减少计算时间。
- 如果批量处理中的所有样本可以并行地处理，那么内存消耗和批量大小成正比。对于多硬件设施，这是批量大小的限制因素。
- 某些硬件上使用特定大小的数组时，运行时间会更少，尤其是GPU，通常使用2的幂数作为批量大小可以更快，如`32,64,128,256`，大模型时尝试用`16`。
- 可能是由于小批量在学习过程中加入了噪声，会带来一些正则化的效果。泛化误差通常在批量大小为1时最好。因为梯度估计的高方差，小批量使用较小的学习率，以保持稳定性，但是降低学习率会使迭代次数增加。<font color="red">ToDo: 最后一个点，我还是不太理解</font>

在实际工程中，我们通常使用小批量梯度下降形式。

### 4.5.3 全批量样本梯度下降 

Full Batch Gradient Descent

样本访问示意图如图4-9所示。

<img src="../Images/4/FullBatch-example.png" />

图4-9  全批量样本访问方式

#### 计算过程

假设一共100个样本，每次使用全部样本：

***
$repeat \lbrace \\\\
  \quad z_1 = x_1 \cdot w + b \\\\ 
  \quad z_2 = x_2 \cdot w + b \\\\ 
  \quad \dots \\\\ 
  \quad z_{100} = x_{100} \cdot w + b \\\\ 
  \quad dw= {1 \over 100}\sum_{i=1}^{100} x_i \cdot (z_i - y_i) \\\\ 
  \quad db= {1 \over 100}\sum_{i=1}^{100} (z_i - y_i) \\\\ 
  \quad w=w-\eta \cdot dw \\\\ 
  \quad b=b-\eta \cdot db \\\\ 
\rbrace$

***

上述算法中，循环体中的前100行分别计算了 $z_1, z_2, ..., z_{100}$，可以换成一次性的矩阵运算。

#### 特点

  - 训练样本：每次使用全部数据集进行一次训练，更新一次梯度，重复以上过程。
  - 优点：受单个样本的影响最小，一次计算全体样本速度快，损失函数值没有波动，到达最优点平稳。方便并行计算。
  - 缺点：数据量较大时不能实现（内存限制），训练过程变慢。初始值不同，可能导致获得局部最优解，并非全局最优解。

#### 运行结果

```Python
if __name__ == '__main__':
    sdr = SimpleDataReader()
    sdr.ReadData()
    params = HyperParameters(1, 1, eta=0.5, max_epoch=1000, batch_size=-1, eps = 0.02)
    net = NeuralNet(params)
    net.train(sdr)
```

设置`batch_size=-1`，即是全批量的意思。

表4-5 全批量样本方式的训练情况

|损失函数值|梯度下降过程|
|---|---|
|<img src="../Images/4/FullBatch-Loss.png"/>|<img src="../Images/4/FullBatch-Trace.png"/>|

表4-5中的右图，梯度下降时，在整个过程中只拐了一个弯儿，就直接到达了中心点。

### 4.5.4 三种方式的比较

表4-6 三种方式的比较

||单样本|小批量|全批量|
|---|---|---|---|
|梯度下降过程图解|<img src="../Images/4/SingleSample-Trace.png"/>|<img src="../Images/4/MiniBatch-Trace.png"/>|<img src="../Images/4/FullBatch-Trace.png"/>|
|批大小|1|10|100|
|学习率|0.1|0.3|0.5|
|迭代次数|304|110|60|
|epoch|3|10|60|
|结果|w=2.003, b=2.990|w=2.006, b=2.997|w=1.993, b=2.998|

表4-6比较了三种方式的结果，从结果看，都接近于 $w=2,b=3$ 的原始解。最后的可视化结果图如图4-10，可以看到直线已经处于样本点比较中间的位置。

<img src="../Images/4/mbgd-result.png" ch="500" />

图4-10 较理想的拟合效果图

相关的概念：

- Batch Size：批大小，一次训练的样本数量。
- Iteration：迭代，一次正向 + 一次反向。
- Epoch：所有样本被使用了一次，叫做一个Epoch，中文的翻译比较杂乱，所以干脆就用原文比较清楚。

<font color="red">ToDo: 这个iteration就是没有做过多的介绍，我还是有点不太理解</font>

假设一共有样本1000个，batch size=20，则一个Epoch中，需要1000/20=50次Iteration才能训练完所有样本。

### 代码位置

ch04, Level5

### 思考与练习

1. 调整学习率、批大小等参数，观察神经网络训练的过程与结果
2. 进一步提高精度（设置`eps`为更小的值），观察`w`和`b`的结果值以及拟合直线的位置 <font color="green">我修改了学习速度eta，batch size之后，然后得到的结果并没有太大的变化，`w`和`b`的结果都是在那个附近左右的了，更细节性的影响，就是需要在等线图上面体现出来的了（就是一个更加直观的过程吧，或者说这一些的参数，就是需要拼人品，或者是用蒙特卡洛的方式进行搜索了，找到一个最佳数值）。但是呢，这个`eps`的精度对训练性能的影响，倒是非常的大的，这个而是非常需要注意的</font>
3. 用纸笔推算一下矩阵运算的维度。假设：
   - $X$ 是 $4\times 2$ 的矩阵
   - $W$ 是 $2\times 3$ 的矩阵
   - $B$ 是 $1\times 3$ 的矩阵

<font color="green">在4.0中，最后一段话是这么写的：**对于 $B$ 来说，它永远是1行，列数与 $W$ 的列数相等。比如 $W$ 是 $3\times 1$ 的矩阵，则 $B$ 是 $1\times 1$ 的矩阵。如果 $W$ 是 $3\times 2$ 的矩阵，意味着3个特征输入到2个神经元上，则 $B$ 是 $1\times 2$ 的矩阵，每个神经元分配1个bias。** 
我的理解，$X$矩阵的行数代表的是样本数，列数代表的是每一个样本所对应的特征值的个数； $W$的行数代表的是每一个样本特征值的个数，然后列数代表神经元的个数（所以，$W$的含义便是，[$W$行数的个数]个特征输入到[$W$列数的个数]个神经元上）； $B$ 的列数代表神经元的个数（
每个神经元[神经元的个数是 $B$ 的列数]分配1个bias[$B$ 的行数永远是1]）。
 </font>

<font color="red">ToDo: $W$ 的列数就是神经元的个数，而是说这个神经元的个数，是怎么定义的。</font>

<font color="red">ToDo: 对于第三个问题，我理解 $X, W, B$ 矩阵的含义，我也是明白 $Z$ 的含义（比如 $Z$ 的维度）；后来我就是过了一下4.3的过程之后，我就是对这个代码的开发过程，就是有了一个了解的了（但是暂时就是没有具体的开发经验）。</font>