<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

# 第十步  个人总结

这个文本的作用，是为以后自己从事人工神经网络的开发做一个备忘录，可以起到快速定位本书内容的作用。

## 知识点解释

1. 关FeedForward Neural Network (FFNN) & CNN中反向传播的误差矩阵&梯度矩阵的计算，参见Section 9.4, 14.1 and 17.3，其实计算误差矩阵，主要目的就是为了辅助计算梯度矩阵。
2. Section 7.1 & 7.2是关于（线性）多分类神经网络正向传播&反向传播的介绍，主要针对的就是softmax多分类函数了。以及Section 3.0, 3.2, 19.5给出了softmax和交叉熵损失函数的关系，以及交叉熵损失函数＆均方差损失函数的区别
3. 在看完LSTM的基本原理（Section 20.1）的内容之后，突然就是明白了，在反向传播的过程中（dnn, rnn & cnn），先是把相应的误差矩阵计算出来，然后就是可以根据误差矩阵，计算出相应的梯度矩阵了，然后这一些梯度矩阵就是可以用来调整weights & biases了。


## 代码示例

### 数据处理代码示例

1. 定时间步RNN需要的数据（从初始的csv到后来的numpy array格式），可以参见Section 19.4相关的内容（ch19, Level4）。Data文件夹中的代码（ch19_PM25.py），就是给出了一个很好的例子（即从csv到最后的numpy's array）。同时呢，PM25DataReader.py里面的Normalize function也是给出了从二维的数据到三维的数据的示例---缺失值、累计值处理，具体的方式可以debug ch19-Level4的代码即可（为了节省时间，需要把max_epoch，时间步长【定义网络时候】调小）。
2. 不定时间步RNN的数据（从txt到numpy array格式），可以参见Section 19.5的相关内容（ch19, Level5）。NameDataReader.py给出了详细的过程，比如读取txt文档，使用try-except-finally，toOneHot的转换，以及list&array的混合使用等。具体的方式可以debug ch19-Level4的代码即可（为了节省时间，需要把max_epoch调小）。
3. ToDo: 读取image数据
4. Section 19.7 使用(单向或双向)rnn，以及MNIST的数据进行训练的时候。原始的数据的维度是$128*1*28*28$（128是batch size），在喂给神经网络进行训练的时候，重新把数据的维度变成了$128*28*28$，这样子，定义rnn网络的时候，时间步也就是28了（另外，此时$128*28*28$的中间一个维度，既可以认为是时间步，也是可以认为是image的width）。参见ch19-Level7的代码给出如何将image数据变成适合rnn的数据（即前一句话提到的维度变化【从四维变成三维】）。----但是，MNIST是灰色的，如果是彩色的，维度变换的时候还是需要注意的。
5. Section 17.2&17.4分别是给出了cnn中，img2col和col2img的实现方式，从而加速cnn的训练速度。另外，还是可以使用numba包将Python的动态编译变成静态编译，进一步加速训练速度。相应的代码请参加ch17-Level2 & ch17-Level4。---另外，对于Pandas的DataFrame，和Numpy的Array，在同样的for循环下，array形式的数据的速度是明显快于dataframe格式的数据。
6. 对数据进行预处理的时候，会有一个专门的工具，即sklearn （同时也是可以参考[链接-1](https://scikit-learn.org/stable/modules/preprocessing.html#)&[链接-2](https://zhuanlan.zhihu.com/p/26444240)）。另外，Section 5.3 & 5.6给出的方法仅仅是一个皮毛，或者就是一个入门而已（至少对于categorical features来说是这样子的）
7. 对于categorcal data，比如可以使用One-Hot encodeing method把数据转换过来，然后直接把这一些数据合并就是可以的了，就是不需要太在意每一列只能够代表一个feature这个概念的了。可以参见预估房价的代码（在Python@GitHub的repository里面）
  
  
Note: 以上的内容，不仅仅是局限于RNN，CNN，或者是CNN，也许对相互通用的。


### 神经网络创建&训练代码示例

1. ch18的代码文件夹里面有基于PyTorch的CNN代码示例，可以借鉴使用的（不过可能由于我的本地配置torchvision的问题，暂时没有办法正常运行起来）
2. ```from tqdm import tqdm```可以用来显示for循环的进度
3. Sections 19.4 & 19.6中的保存最小值loss的代码，就是有极大的好处的了，可以避免重复训练的一个烦恼了。（19.4节有较为详细的comment，19.4&19.6节这部分的代码，其实都是一样的。）
4. Section 19.4 & 19.6解释RNN预测未来数据的逻辑
5. Section 14.0就是有一个Pytorch的简易框架图。
6. ch06中有线性分类的pytorch代码，ch10, 11, 12中有非线性分类的Pytorch代码, 
7. Section 15.6有关于使用激活函数的一些的个人总结。


### 图片plot代码示例

1. 画1x2（或者类似）的图片，可以参见TrainingHistory_3_0.py中的ShowLossHistory函数，同时呢，HyperParameters_4_3.py的toString函数也是给出了title的便捷方式了。具体的方式可以debug ch19-level4的代码即可（运行代码的时候，请把line 54的num_step设置为4，可以加快debug的速度） 
2. 混淆矩阵（confusion matrix）的一个很好的应用，就似乎在多分类的问题中，可以让结果变得一目了然。Section 19.5 以及ch19-level5是一个很好的例子
