<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 19.6 深度循环神经网络

### 19.6.1 深度循环神经网络的结构图

前面的几个例子中，单独看每一时刻的网络结构，其实都是由“输入层->隐层->输出层”所组成的，这与我们在前馈神经网络中学到的单隐层的知识一样，由于输入层不算做网络的一层，输出层是必须具备的，所以网络只有一个隐层。我们知道单隐层的能力是有限的，所以人们会使用更深（更多隐层）的网络来解决复杂的问题。<font color=green> 其实，我的理解，就是在写作的时候，明确一下，比如说就是说明白网络是由一层输入层，多少个隐藏层，以及一个输出层构成的，这样子就是可以避免别人猜测了。但是我觉得我现在读的神经网络的文章还是太少了</font>

在循环神经网络中，会有同样的需求，要求每一时刻的网络是由多个隐层组成。比如图19-20为两个隐层的循环神经网络，用于解决和19.4节中的同样的问题。

<img src="../Images/19/deep_rnn_net.png"/>

图19-20 两个隐层的循环神经网络

注意图19-20中最左侧的两个隐藏状态s1和s2是同时展开为右侧的图的，
这样的循环神经网络称为深度循环神经网络，它可以具备比单隐层的循环神经网络更强大的能力。<font color=green> 我的天，两个隐藏层的RNN，就可以称之为Deep RNN了。注意一下$W1$，$W2$的位置，以及箭头指向。</font>

### 19.6.2 前向计算

#### 公式推导

对于第一个时间步：
$$
h1 = x \cdot U \tag{1}
$$
$$
h2 = s1 \cdot Q \tag{2}
$$

对于后面的时间步：
$$
h1 = x \cdot U + s1_{t-1} \cdot W1 \tag{3}
$$

$$
h2 = s1 \cdot Q + s2_{t-1} \cdot W2 \tag{4}
$$

对于所有的时间步：

$$
s1 = \tanh(h1) \tag{5}
$$

$$
s2 = \tanh(h2) \tag{6}
$$

对于最后一个时间步：
$$
z = s2 \cdot V \tag{7}
$$
$$
a = Identity(z) \tag{8}
$$
$$
Loss = loss_{\tau} = \frac{1}{2} (a-y)^2 \tag{9}
$$

由于是拟合任务，所以公式8的Identity()函数只是简单地令a=z，以便统一编程接口，最后用均方差做为损失函数。

注意并不是所有的循环神经网络都只在最后一个时间步有监督学习信号，而只是我们这个问题需要这样。在19.2节中的例子就是需要在每一个时间步都要有输出并计算损失函数值的。所以，公式9中只计算了最后一个时间步的损失函数值，做为整体的损失函数值。<font color=red>这个就是我一直困惑的，到底是在什么时候，就是所有的时间步都是输出，到底是什么时候，就是在最后一个时间步输出，这个问题，可以就是在看完这一章之后，稍微研究一下的了。</font>

#### 代码实现

注意前向计算时需要把prev_s1和prev_s2传入，即上一个时间步的两个隐层的节点值（矩阵）。

```Python
class timestep(object):
    def forward(self, x, U, V, Q, W1, W2, prev_s1, prev_s2, isFirst, isLast):
        ...
```

### 19.6.3 反向传播

#### 公式推导

反向传播部分和前面章节的内容大致相似，我们只把几个关键步骤直接列出来，不做具体推导：

对于最后一个时间步：
$$
\frac{\partial Loss}{\partial z} = a-y \rightarrow dz \tag{10}
$$

$$
\frac{\partial Loss}{\partial V}=\frac{\partial Loss}{\partial z}\frac{\partial z}{\partial V}=s2^{\top} \cdot dz \rightarrow dV \tag{11}
$$

$$
\begin{aligned}
\frac{\partial Loss}{\partial h2} &= \frac{\partial Loss}{\partial z}\frac{\partial z}{\partial s2}\frac{\partial s2}{\partial h2}
\\\\
&=(dz \cdot V^{\top}) \odot \sigma'(s2) \rightarrow dh2 
\end{aligned}
\tag{12}
$$

$$
\begin{aligned}
\frac{\partial Loss}{\partial h1} &= \frac{\partial Loss}{\partial h2}\frac{\partial h2}{\partial s1}\frac{\partial s1}{\partial h1} \\\\
&=(dh2 \cdot Q^{\top}) \odot \sigma'(s1) \rightarrow dh1 
\end{aligned}
\tag{13}
$$

对于其他时间步：

$$
dz = 0 \tag{14}
$$

$$
\begin{aligned}
\frac{\partial Loss}{\partial h2_t} &= \frac{\partial Loss}{\partial h2_{t+1}}\frac{\partial h2_{t+1}}{\partial s2_t}\frac{\partial s2_t}{\partial h2_t}
\\\\
&=(dh2_{t+1} \cdot W2^{\top}) \odot \sigma'(s2_t) \rightarrow dh2_t
\end{aligned}
\tag{15}
$$

$$
dV = 0 \tag{16}
$$

$$
\begin{aligned}
\frac{\partial Loss}{\partial h1_t} &= \frac{\partial Loss}{\partial h1_{t+1}}\frac{\partial h1_{t+1}}{\partial s1_t}\frac{\partial s1_t}{\partial h1_t}+\frac{\partial loss_t}{\partial h2_t}\frac{\partial h2_t}{\partial s1_t}\frac{\partial s1_t}{\partial h1_t}
\\\\
&=(dh1_{t+1} \cdot W1^{\top} + dh2_t\cdot Q^{\top}) \odot \sigma'(s1_t) \rightarrow dh1_t
\end{aligned}
\tag{17}
$$

对于第一个时间步：

$$
dW1 = 0, dW2 = 0 \tag{18}
$$

对于其他时间步：

$$
\frac{\partial Loss}{\partial W1}=s1^{\top}_ {t-1} \cdot dh_1 \rightarrow dW1 \tag{19}
$$

$$
\frac{\partial Loss}{\partial W2}=s2^{\top}_ {t-1} \cdot dh2 \rightarrow dW2 \tag{20}
$$

对于所有时间步：

$$
\frac{\partial Loss}{\partial Q}=\frac{\partial Loss}{\partial h2}\frac{\partial h2}{\partial Q}=s1^{\top} \cdot dh2 \rightarrow dQ \tag{21}
$$

$$
\frac{\partial Loss}{\partial U}=\frac{\partial Loss}{\partial h1}\frac{\partial h1}{\partial U}=x^{\top} \cdot dh1 \rightarrow dU \tag{22}
$$

<font color=green> 其实公式21，21使用这个Loss也不算是错的，只不过作者可能想是表达一下所有的时间步的概念，但是在真正代码实现的时候，就是需要每一个时间步都是分开算这个梯度矩阵&误差矩阵的了，然后最后再相加起来了。</font>

#### 代码实现

```Python
class timestep(object):
    def backward(self, y, prev_s1, prev_s2, next_dh1, next_dh2, isFirst, isLast):
        ...
```

### 19.6.4 运行结果

#### 超参设置

我们搭建一个双隐层的循环神经网络，隐层1的神经元数为2，隐层2的神经元数也为2，其它参数保持与单隐层的循环神经网络一致：<font color=green> 这里也是间接的表明了，图19-20的$x$, $h1$, $h2$代表的不是一个神经元，可能是多个神经元，这里只不过是做了简化处理的了。</font>

- 网络类型：回归
- 时间步数：24
- 学习率：0.05
- 最大迭代数：100
- 批大小：64
- 输入特征数：6
- 输出维度：1

#### 训练结果

训练过程如图19-21所示，训练结果如表19-10所示。

<img src="../Images/19/deep_rnn_loss.png"/>

图19-21 训练过程中的损失函数值和准确度的变化

表19-10 预测时长与准确度的关系

|预测时长|结果|预测结果|
|---|---|---|
|8|损失函数值：<br/>0.001157<br/>准确度：<br/>0.740684|<img src="../Images/19/deeprnn_pm25_fitting_result_24_8.png" height="240"/>
|4|损失函数值：<br/>0.000644<br/>准确度：<br/>0.855700|<img src="../Images/19/deeprnn_pm25_fitting_result_24_4.png" height="240"/>
|2|损失函数值：<br/>0.000377<br/>准确度：<br/>0.915486|<img src="../Images/19/deeprnn_pm25_fitting_result_24_2.png" height="240"/>
|1|损失函数值：<br/>0.000239<br/>准确度：<br/>0.946411|<img src="../Images/19/deeprnn_pm25_fitting_result_24_1.png" height="240"/>

#### 与单层循环神经网络的比较

对于19.3节中的单层循环神经网络，参数配置如下：
```
U: 6x4+4=28
V: 4x1+1= 5
W: 4x4  =16
-----------
Total:   49
```

对于两层的循环神经网络来说，参数配置如下：

```
U: 6x2=12
Q: 2x2= 4
V: 2x1= 2
W1:2x2= 4
W2:2x2= 4
---------
Total: 26
```

表19-11 预测结果比较

||单隐层循环神经网络|深度（双层）循环神经网络|
|---|---|---|
|参数个数|49|26|
|损失函数值（8小时）|0.001171|0.001157|
|损失函数值（4小时）|0.000686|0.000644|
|损失函数值（2小时）|0.000414|0.000377|
|损失函数值（1小时）|0.000268|0.000239|
|准确率值（8小时）|0.737769|0.740684|
|准确率值（4小时）|0.846447|0.855700|
|准确率值（2小时）|0.907291|0.915486|
|准确率值（1小时）|0.940090|0.946411|

从表19-11可以看到，双层的循环神经网络在参数少的情况下，取得了比单层循环神经网络好的效果。

### 代码位置

ch19, Level6

<font color=red> 为什么代码中，时间步长是24呢，这个又是怎么确定的呢？--> 在Section 19.5中，时间步长n=19，可以容纳19个字母的单词。换句话说，每一个字母，就是对应的是一个时间步，然后每一个时间步里面，又是因为使用了OneHot的形式，所有输入层就是需要26个神经元。另外，从Section 19.1 & 19.2的例子也是可以看到，这个时间步长，应该就是和数据的维度有很大的关系的。---> 我明白了，原始的样本数据的样本数就是很多的了，然后作者作者就是自己定义成为了24个，没有什么特别的原因了。这个就是和Sections 19.1, 19.2, 19.5的内容是不一样的，因为这几个例子中的时间步都是有特定的含义的，比如说在Section 19.5中，时间步代表的则是每一个名字中，字母的个数。</font>

<font color=red> 同时，这里就是继续回应一下前面Section 19.4中关于预测未来8，4，2，1小时数据的疑惑。首先，在代码中，时间步是设置为24，在实际意义中，这个24代表的是24小时的数据，或者说就是一整天的数据作为一个完成的样本，然后样本数据就是从小时的维度扩展到天的维度了（这里也是很好的回应了上面的24个时间步的物理含义）。同时的呢，这个预测未来的小时数，从现有的网络来看，应该是小于这个总的时间步的。

--> 就算是我这一次再过一次这个prediction部分的代码，还是不太明白这背后的含义，仅仅是明白了这个逻辑。比如说，我是需要预测未来4小时的数据，

在i=0(line 315的predict函数中)的时候，X(含24个时间步的样本)的数值不做调整，获得A[0]
在i=1(line 315的predict函数中)的时候，X(含24个时间步的样本)的[index=23]=A[0]，获得A[1]
在i=2(line 315的predict函数中)的时候，X(含24个时间步的样本)的[index=22]=A[0], [index=23]=A[1]，获得A[2]
在i=3(line 315的predict函数中)的时候，X(含24个时间步的样本)的[index=21]=A[0], [index=22]=A[1], [index=23]=A[2]，获得A[3]

我不太明白，
1. 既然是预测未来4小时的数据，为什么需要四个样本，---> 我的理解，没有啥实际含义了
2. 除了需要4个样本之外，为什么每一个样本里面，都是需要预测4次。---> 这个也许是可以理解的，预测未来4小时，其意思就是说预测未来4个小时，并且每一个小时的PM2.5的数值。
3. 为什么需要对(含24个时间步)的样本X做修改的。---> 基于第二点，比如我们预测了第0个小时的数据，然后接着我们就是需要更新第0个小时pm25的数据，然后基于这个数据，再继续预测第1个小时的数据。同时就是需要更新两次数据（即第22，23次），接着就是一次类推了
4. 之所以是从23, 22, 21, ..., 0这样子的顺序更新。可以这么理解，数据从max index往min index更新的了，可以想象一下，如果预测完24小时的数据之后，index=0代表的是最先得一次预测(24小时之前的预测结果)，而index=23代表的最近一次的预测。</font>

<font color=green> 注意一下，这个代码中，也是没有使用任何的bias这个parameters的</font>


