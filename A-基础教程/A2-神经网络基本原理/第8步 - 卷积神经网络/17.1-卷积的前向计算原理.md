<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 17.1 卷积的前向计算

### 17.1.1 卷积的数学定义

#### 连续定义

$$h(x)=(f*g)(x) = \int_{-\infty}^{\infty} f(t)g(x-t)dt \tag{1}$$

卷积与傅里叶变换有着密切的关系。利用这点性质，即两函数的傅里叶变换的乘积等于它们卷积后的傅里叶变换，能使傅里叶分析中许多问题的处理得到简化。<font color=red> ToDo: 虽然作者就是提到了这个卷积和傅里叶变换的关系，但是还是不够明确，得再找文献了解卷积&傅里叶变换在CNN中的原理</font>

#### 离散定义

$$h(x) = (f*g)(x) = \sum^{\infty}_{t=-\infty} f(t)g(x-t) \tag{2}$$

### 17.1.2 一维卷积实例

有两枚骰子$f,g$，掷出后二者相加为4的概率如何计算？

第一种情况：$f(1)g(3), 3+1=4$，如图17-9所示。

<img src="../Images/17/touzi1.png" />

图17-9 第一种情况

第二种情况：$f(2)g(2), 2+2=4$，如图17-10所示。

<img src="../Images/17/touzi2.png" />

图17-10 第二种情况

第三种情况：$f(3)g(1), 1+3=4$，如图17-11所示。

<img src="../Images/17/touzi3.png" />

图17-11 第三种情况

因此，两枚骰子点数加起来为4的概率为：

$$
\begin{aligned}
h(4) &= f(1)g(3)+f(2)g(2)+f(3)g(1) \\\\
&=f(1)g(4-1) + f(2)g(4-2) + f(3)g(4-3)
\end{aligned}
$$

符合卷积的定义，把它写成标准的形式就是公式2：

$$h(4)=(f*g)(4)=\sum _{t=1}^{3}f(t)g(4-t)$$

<font color=green> 对比公式（2）中$f(t)g(x-t)$的定义，就是可以知道$x-t + t = x$，类似的，而$x$在这个一维卷积的例子中，等于4（这个数值也是例子中给出的）。</font>

<font color=red> ToDo: 这个以为卷积的例子，我暂时还是没有办法和公式（2）结合在一起。----> 我大概是明白了，比如说, 在g(x-t)中，因为t是负数，就是翻转的意思，所以就是需要从末尾开始算起（从右往左，而不是从左往右-->这个是对于f(x)），然后因为x=4，所以第一个数其实就是g=3的时候，所以g(4-1)=3；然后以此类推g(4-2)=2；g(4-3)=1；</font>

### 17.1.3 单入单出的二维卷积

二维卷积一般用于图像处理上。在二维图片上做卷积，如果把图像Image简写为$I$，把卷积核Kernal简写为$K$，则目标图片的第$(i,j)$个像素的卷积值为：

$$
h(i,j) = (I*K)(i,j)=\sum_m \sum_n I(m,n)K(i-m,j-n) \tag{3}
$$

可以看出，这和一维情况下的公式2是一致的。从卷积的可交换性，我们可以把公式3等价地写作：

$$
h(i,j) = (I*K)(i,j)=\sum_m \sum_n I(i-m,j-n)K(m,n) \tag{4}
$$

公式4的成立，是因为我们将Kernal进行了翻转<font color=red>（ToDo: 我不太明白这个翻转是什么意思，但是上面的这个公式，我倒是可以理解的，因为这个毕竟是卷积的一个性质，--->明白了，这里的翻转的意思，不是使用比如原来的m，n做这个点乘的运算了，而是换成了使用i-m，j-n来做运算 --> 参考上面一个红色的备注，就是可以明白翻转的意思了。  同时呢，公式（3）中的“目标图片的第$(i,j)$个像素的卷积值为”有点晦涩难懂，但是理解成公式（4）的时候，就是轻松很多了。）</font>。在神经网络中，一般会实现一个互相关函数(corresponding function)，而卷积运算几乎一样，但不反转Kernal：

$$
h(i,j) = (I*K)(i,j)=\sum_m \sum_n I(i+m,j+n)K(m,n) \tag{5}
$$

在图像处理中，自相关函数和互相关函数定义如下：

- 自相关：设原函数是f(t)，则$h=f(t) \star f(-t)$，其中$\star$表示卷积
- 互相关：设两个函数分别是f(t)和g(t)，则$h=f(t) \star g(-t)$

互相关函数的运算，是两个序列滑动相乘，两个序列都不翻转<font color=blue>（滑动相乘，不翻转见公式5）</font>。卷积运算也是滑动相乘，但是其中一个序列需要先翻转，再相乘<font color=blue>（滑动相乘，翻转见公式4，关键点就是在于这个减号$-$上）</font>。所以，从数学意义上说，机器学习实现的是互相关函数，而不是原始含义上的卷积。但我们为了简化，把公式5也称作为卷积。这就是卷积的来源。<font color=blue> 这个“滑动相乘”的含义，在数学表达上，就是公式（5）中的两个sum符号。</font>

结论：

1. 我们实现的卷积操作不是原始数学含义的卷积，而是工程上的卷积，可以简称为卷积
2. 在实现卷积操作时，并不会反转卷积核

<font color=blue> 这里的总结就是非常的棒的，给出了神经网络中的卷积，和传统的意义上的卷积的区别。</font> <font color=red> ToDo: 另外，这个公式（5）中的$m$， $n$应该是卷积核的维度，---> 这个理解是正确，同时呢，$m$和$n$的数值相等，并且是一个奇数（不要问我为什么，书上这么写的，我暂时也是不清楚原因。）</font>

在传统的图像处理中，卷积操作多用来进行滤波，锐化或者边缘检测啥的。我们可以认为卷积是利用某些设计好的参数组合（卷积核）去提取图像空域上相邻的信息。<font color=blue> 换句话说，卷积就是使用设定好的参数（比如Section 17.0的参数），来提取图像中某一些的信息（即特征）。</font>

按照公式5，我们可以在4x4的图片上，用一个3x3的卷积核，通过卷积运算得到一个2x2的图片，运算的过程如图17-12所示。

<img src="../Images/17/conv_w3_s1.png" ch="526" />

图17-12 卷积运算的过程

<font color=green> 没有想到，这个NN中的卷积的概念，是element-wise的乘法，而不是传统意义上的矩阵相乘。</font>

### 17.1.4 单入多出的升维卷积

原始输入是一维的图片，但是我们可以用多个卷积核分别对其计算，从而得到多个特征输出。如图17-13所示。

<img src="../Images/17/conv_2w3.png" ch="500" />

图17-13 单入多出的升维卷积

一张4x4的图片，用两个卷积核并行地处理，输出为2个2x2的图片。在训练过程中，这两个卷积核会完成不同的特征学习。<font color=blue>这里提到的非常的棒，不同的卷积核，是提取不同的特征。</font>

### 17.1.5 多入单出的降维卷积

一张图片，通常是彩色的，具有红绿蓝三个通道。我们可以有两个选择来处理：

1. 变成灰度的，每个像素只剩下一个值，就可以用二维卷积<font color=green>【因为是彩色的图，所以就是变成三个通道，而根据Section 17.1.3的二维来看，这里的多维的概念，就是变成了三个通道，就是从单一的灰色变成了彩色（R，G，B）】</font>
2. 对于三个通道，每个通道都使用一个卷积核，分别处理红绿蓝三种颜色的信息

显然第2种方法可以从图中学习到更多的特征，于是出现了三维卷积，即有三个卷积核分别对应书的三个通道，三个子核的尺寸是一样的，比如都是2x2，这样的话，这三个卷积核就是一个3x2x2的立体核，称为过滤器Filter，所以称为三维卷积。<font color=blue> 这里就是给出了filter的概念了。</font>

<img src="../Images/17/multiple_filter.png" />

图17-14 多入单出的降维卷积

在上图中，每一个卷积核对应着左侧相同颜色的输入通道，三个过滤器的值并不一定相同。对三个通道各自做卷积后，得到右侧的三张特征图，然后再按照原始值不加权地相加在一起，得到最右侧的白色特征图，这张图里面已经把三种颜色的特征混在一起了，所以画成了白色，表示没有颜色特征了。

虽然输入图片是多个通道的，或者说是三维的，但是在相同数量的过滤器的计算后，相加在一起的结果是一个通道，即2维数据，所以称为降维。这当然简化了对多通道数据的计算难度，但同时也会损失多通道数据自带的颜色信息。

<font color=blue>这里就是有几点需要注意：1. 就是每一个kernel是对应的一个channel（这一点非常的重要），需要着重看图17.14左边部分的箭头指向；2. 这个的aggregation的方式是直接是把三个通道得到的结果相加，这个不是唯一的方式，但是作为一个示例，是足够了。</font>

### 17.1.6 多入多出的同维卷积

在上面的例子中，是一个过滤器Filter内含三个卷积核Kernal<font color=blue>（图17.14中橙色部分的三个kernal称之为一个过滤器filter）</font>。我们假设有一个彩色图片为3x3的，如果有两组3x2x2的卷积核的话，会做什么样的卷积计算？看图17-15。

<img src="../Images/17/conv3dp.png" ch="500" />

图17-15 多入多出的卷积运算

第一个过滤器Filter-1为棕色所示，它有三卷积核(Kernal)，命名为Kernal-1，Keanrl-2，Kernal-3，分别在红绿蓝三个输入通道上进行卷积操作，生成三个2x2的输出Feature-1,n。然后三个Feature-1,n相加，并再加上b1偏移值，形成最后的棕色输出Result-1。

对于灰色的过滤器Filter-2也是一样，先生成三个Feature-2,n，然后相加再加b2，最后得到Result-2。

之所以Feature-m,n还用红绿蓝三色表示，是因为在此时，它们还保留着红绿蓝三种色彩的各自的信息，一旦相加后得到Result，这种信息就丢失了。<font color=blue>（这一句话就是需要注意一下，1. 公式5中的$m$，$n$和此处的$m$，$n$是不同的概念（公式5中的是一个kernal的维度）。而在这里，$m$代表是第$m$个filter，然后$n$代表的是第$n$个channel，或者是第$n$个kernal；同时，这里也是给出了什么时候会出现这个下表$m$，$n$，什么时候这个下表$m$，$n$会消失，也是需要注意一下的）</font>。

### 17.1.7 卷积编程模型

上图侧重于解释数值计算过程，而图17-16侧重于解释五个概念的关系：

- 输入 Input Channel
- 卷积核组 WeightsBias
- 过滤器 Filter
- 卷积核 kernal
- 输出 Feature Map

<img src="../Images/17/conv3d.png" ch="500" />

图17-16 三通道经过两组过滤器的卷积过程

在此例中，输入是三维数据（3x32x32），经过2x3x5x5的卷积后，输出为三维（2x28x28），维数并没有变化，只是每一维内部的尺寸有了变化，一般都是要向更小的尺寸变化，以便于简化计算。<font color=blue>（这里涉及到的维度数据，是需要结合图17.16做仔细的阅读的，比如这个kernal中的(2x3)的含义，以及(5x5)的含义，之所以这么写（2x3x5x5）的原因，比如说2代表的两个filter，3代表么一个filter有3个kernal，然后28x28就是代表每一个kernal的维度）。</font>

<font color=blue>另外，计算维度的时候，都是先是从高维度算起，比如说先是从filter，然后是channel数量；最后是kernal数量。</font>

对于三维卷积，有以下特点：

1. 预先定义输出的feature map的数量，而不是根据前向计算自动计算出来，此例中为2，这样就会有两组WeightsBias
2. 对于每个输出，都有一个对应的过滤器Filter，此例中Feature Map-1对应Filter-1
3. 每个Filter内都有一个或多个卷积核Kernal，对应每个输入通道(Input Channel)，此例为3，对应输入的红绿蓝三个通道
4. 每个Filter只有一个Bias值，Filter-1对应b1，Filter-2对应b2
5. 卷积核Kernal的大小一般是奇数如：1x1, 3x3, 5x5, 7x7等，此例为5x5

<font color=blue> 上面的这个总结就是非常的好的，就是先确定feacture map的数量，然后就是可以确定filter的数量了；然后接着就是可以根据这个样本中通道的数量，就是可以确定每一个filter里面是会有几个kernal了；另外，每一个filter就是会对应一个bias；最后对于kerinal，一般都是采用的是奇数的形式（不过我不清楚为什么是奇数的）。</font>

对于上图，我们可以用在全连接神经网络中的学到的知识来理解：

1. 每个Input Channel就是特征输入，在上图中是3个
2. 卷积层的卷积核相当于隐层的神经元，上图中隐层有2个神经元 <font color=red> ToDo: 这里应该是笔误，就是每一个filter对应于DL中的一个神经元 ，然后上图中就是有两个filter（两个神经元）</font>
3. $W(m,n), m=[1,2], n=[1,3]$相当于隐层的权重矩阵$w_{11},w_{12},......$ <font color=red> toDo: $m$， $n$的数值应该是等于$m=[1,1], n=[1,2]$，即第一隐藏层的第一个filter，第一，和第二个kernal</font>
4. 每个卷积核（神经元）有1个偏移值 <font color=red> ToDo: 这里应该是笔误，每个filter就是有一个偏移值。</font>

### 17.1.8 步长 stride

前面的例子中，每次计算后，卷积核会向右或者向下移动一个单元，即步长stride = 1。而在图17-17这个卷积操作中，卷积核每次向右或向下移动两个单元，即stride = 2。

<img src="../Images/17/Stride2.png" />

图17-17 步长为2的卷积

在后续的步骤中，由于每次移动两格，所以最终得到一个2x2的图片。

<font color=red> 所以这个步长stride可以是1，或者是2，或者是其他数据了？但是应该是小于keirnal的维度的，要不然就是会出现feature的丢失了，因为就是不能够乘到kernal了。</font>

### 17.1.9 填充 padding

如果原始图为4x4，用3x3的卷积核进行卷积后，目标图片变成了2x2。如果我们想保持目标图片和原始图片为同样大小，该怎么办呢？一般我们会向原始图片周围填充一圈0，然后再做卷积。如图17-18。

<img src="../Images/17/padding.png" ch="500" />

图17-18 带填充的卷积

### 17.1.10 输出结果

综合以上所有情况，可以得到卷积后的输出图片的大小的公式：

$$
H_{Output}= {H_{Input} - H_{Kernal} + 2Padding \over Stride} + 1
$$

$$
W_{Output}= {W_{Input} - W_{Kernal} + 2Padding \over Stride} + 1
$$

以图17-17为例：

$$H_{Output}={5 - 3 + 2 \times 0 \over 2}+1=2$$

以图17-18为例：

$$H_{Output}={4 - 3 + 2 \times 1 \over 1}+1=4$$

两点注意：

1. 一般情况下，我们用正方形的卷积核，且为奇数
2. 如果计算出的输出图片尺寸为小数，则取整，不做四舍五入

<font color=blue> 这两个注意事项还是非常重要的，</font><font color=red>ToDo: 但是这个$H_{Output}$， $W_{Output}$的含义有点模糊，我的理解就是输出的feature map，feature channel（就是每一个channel和一个kernal卷积后的结果的维度）----> 这两个代表的是kernal的高度和宽度</font>