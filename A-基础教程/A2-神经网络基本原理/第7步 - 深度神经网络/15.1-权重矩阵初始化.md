<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 15.1 权重矩阵初始化

权重矩阵初始化是一个非常重要的环节，是训练神经网络的第一步，选择正确的初始化方法会带了事半功倍的效果。这就好比攀登喜马拉雅山，如果选择从南坡登山，会比从北坡容易很多。而初始化权重矩阵，相当于下山时选择不同的道路，在选择之前并不知道这条路的难易程度，只是知道它可以抵达山下。这种选择是随机的，即使你使用了正确的初始化算法，每次重新初始化时也会给训练结果带来很多影响。

比如第一次初始化时得到权重值为(0.12847，0.36453)，而第二次初始化得到(0.23334，0.24352)，经过试验，第一次初始化用了3000次迭代达到精度为96%的模型，第二次初始化只用了2000次迭代就达到了相同精度。这种情况在实践中是常见的。

<font color=green> 这个section看完之后，才是突然是意识到，这里都是谈的是weight的初始化，并没有提到bias的初始化。</font>

### 15.1.1 零初始化

即把所有层的`W`值的初始值都设置为0。

$$
W = 0
$$

但是对于多层网络来说，绝对不能用零初始化，否则权重值不能学习到合理的结果。看下面的零值初始化的权重矩阵值打印输出：
```
W1= [[-0.82452497 -0.82452497 -0.82452497]]
B1= [[-0.01143752 -0.01143752 -0.01143752]]
W2= [[-0.68583865]
 [-0.68583865]
 [-0.68583865]]
B2= [[0.68359678]]
```

可以看到`W1`、`B1`、`W2`内部3个单元的值都一样，这是因为初始值都是0，所以梯度均匀回传，导致所有`W`的值都同步更新，没有差别。这样的话，无论多少轮，最终的结果也不会正确。

<font color=green> 也就是说，这个weight是不能够设定成为0的，要不然，最终得到的weight（对于同一层hidden layer来说），都是会一样的。</font> <font color=red>但是，为什么会是一样，这个就是一件非常有意思的问题了，文中也是没有给出来一个原因，就有待我自己去挖掘了。</font> -->
<font color=green> 后来我在知乎上找到了一个解释： 为什么将所有W初始化为0是错误的呢？是因为如果所有的参数都是0，那么所有神经元的输出都将是相同的，那在back propagation的时候同一层内所有神经元的行为也是相同的 --- gradient相同，weight update也相同。这显然是一个不可接受的结果。</font>

### 15.1.2 标准初始化

标准正态初始化方法保证激活函数的输入均值为0，方差为1。<font color=green>（注意，这里是激活函数的输入）</font>将W按如下公式进行初始化：

$$
W \sim N \begin{bmatrix} 0, 1 \end{bmatrix}
$$

其中的W为权重矩阵，N表示高斯分布，Gaussian Distribution，也叫做正态分布，Normal Distribution，所以有的地方也称这种初始化为Normal初始化。

一般会根据全连接层的输入和输出数量来决定初始化的细节：

$$
W \sim N
\begin{pmatrix} 
0, \frac{1}{\sqrt{n_{in}}}
\end{pmatrix}
$$ 
<font color=green> 上述公式为正态分布均匀分布</font>

$$
W \sim U
\begin{pmatrix} 
-\frac{1}{\sqrt{n_{in}}}, \frac{1}{\sqrt{n_{in}}}
\end{pmatrix}
$$ 
<font color=green> 上述公式为均匀分布</font>

<font color=green> 根据这里的描述，对于全链接层的input来说，总共就是有$n_{in}$个的了。</font>

当目标问题较为简单时，网络深度不大，所以用标准初始化就可以了。但是当使用深度网络时，会遇到如图15-1所示的问题。

<img src="../Images/15/init_normal_sigmoid.png" ch="500" />

图15-1 标准初始化在Sigmoid激活函数上的表现

图15-1是一个6层的深度网络，使用全连接层+Sigmoid激活函数，图中表示的是各层激活函数的直方图。可以看到各层的激活值严重向两侧[0,1]靠近，从Sigmoid的函数曲线可以知道这些值的导数趋近于0，反向传播时的梯度逐步消失。处于中间地段的值比较少，对参数学习非常不利。

<font color=green>我从图15.1看到的是，这个对于每一层的激活函数来说，大部分的数值都是0或者是1出现的了。对应sigmoid的导数图的话，就是可以知道，大部分的导数值都是趋近于（对于sigmoid的导数来说，中间的数值最大，两边的数值是最小的，且逼近到0，see section 8.1）。</font>

<font color=green>另外，从这段话中，就是明白了，原来对于激活函数的输入值来说，最好大部分是在中间的了，这样子便于反向传播的需要。其实也是可以理解的，如果都是大部分在两边的话，反向传播的结果就是不会特别的理想的了。</font>

### 15.1.3 Xavier初始化方法

基于上述观察，Xavier Glorot等人研究出了下面的Xavier$^{[1]}$初始化方法。

条件：正向传播时，激活值的方差保持不变；反向传播时，关于状态值的梯度的方差保持不变。<font color=red>ToDo: 激活值指的是激活函数的输出值吗？那状态值的梯度，就是反向传播过程中，就是相应weihgts和biases的梯度（导数）意思吗？</font>

$$
W \sim N
\begin{pmatrix}
0, \sqrt{\frac{2}{n_{in} + n_{out}}} 
\end{pmatrix}
$$

$$
W \sim U 
\begin{pmatrix}
 -\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}} 
\end{pmatrix}
$$

其中的W为权重矩阵，N表示正态分布（Normal Distribution），U表示均匀分布（Uniform Distribution)。下同。<font color=red>ToDo: 很有意思的一个问题呀，上面的标准初始化方法，也是有正态分布，和均匀分布两种公式的，然后这边也是正态分布和均匀分布两个公式，那权重矩阵是满足哪一种分布呢?</font> 

假设激活函数关于0对称，且主要针对于全连接神经网络。适用于tanh和softsign。

即权重矩阵参数应该满足在该区间内的均匀分布。其中的W是权重矩阵，U是Uniform分布，即均匀分布。<font color=red>ToDo: 意思就是说，如果我的这个激活函数是关于0对称（即tanh），对于这一种情况来说，权重初始化的方式，那也最好就是在这个区间是均匀分布的？</font>

论文摘要：神经网络在2006年之前不能很理想地工作，很大原因在于权重矩阵初始化方法上。Sigmoid函数不太适合于深度学习，因为会导致梯度饱和。基于以上原因，我们提出了一种可以快速收敛的参数初始化方法。<font color=green>终于知道在chapter 14中的所有真实例子，为什么不适用sigmoid函数，而是使用ReLU了，原来就是在深度网路里面，容易出现梯度饱和的情况。</font> <font color=red>那这个梯度饱和，到底是一个什么意思呢？</font> <font color=blue> 虽然说sigmoid不适用于深度学习，根据最后一张表格15.2，是可以使用Tanh进行提到的</font>

Xavier初始化方法比直接用高斯分布进行初始化W的优势所在： 

一般的神经网络在前向传播时神经元输出值的方差会不断增大，而使用Xavier等方法理论上可以保证每层神经元输入输出方差一致。<font color=green>见图15.2，就是可以看出，这个网络的激活值出现的分布是差不多的（y轴代码的是出现的相应数值的次数）。</font> <font color=blue> -----> 这里，就是解释了，上面条件中的描述：正向传播时，激活值的方差保持不变 </font>  

图15-2是深度为6层的网络中的表现情况，可以看到，后面几层的激活函数输出值的分布仍然基本符合正态分布，利于神经网络的学习。<font color=green>其实这个也是可以理解的，因为sigmoid的导数函数就是在0点附近的时候，是最优的。所以激活函数的输出值在0点附近，也是我们想要的。</font> 

<img src="../Images/15/init_xavier_sigmoid.png" ch="500" />

图15-2 Xavier初始化在Sigmoid激活函数上的表现

表15-1 随机初始化和Xavier初始化的各层激活值与反向传播梯度比较 <font color=green>这里的这张对比表格，就是可以看出，随着网络层数的增加，这个正态分布就是不停的收窄的了（如果使用随机初始化），但是呢，如果是使用Xavier的方式，这个正太分布，就是会保持基本一致。从而也是验证了，Xavier方式在sigmoid情况下的优势。</font> <font color=blue> -----> 这里，就是解释了，上面条件中的描述：反向传播时，关于状态值的梯度的方差保持不变，从表中就是可以看到，这个状态值的梯度也是基本保持不变的 </font>  

| |各层的激活值|各层的反向传播梯度|
|---|---|---|
| 随机初始化 |<img src="..\Images\15\forward_activation1.png"><br/>激活值分布渐渐集中|<img src="..\Images\15\backward_activation1.png"><br/>反向传播力度逐层衰退|
| Xavier初始化 |<img src="..\Images\15\forward_activation2.png"><br/>激活值分布均匀|<img src="..\Images\15\backward_activation2.png"><br/>反向传播力度保持不变|

但是，随着深度学习的发展，人们觉得Sigmoid的反向力度受限，又发明了ReLU激活函数。图15-3显示了Xavier初始化在ReLU激活函数上的表现。

<img src="../Images/15/init_xavier_relu.png" ch="500" />

图15-3 Xavier初始化在ReLU激活函数上的表现

可以看到，随着层的加深，使用ReLU时激活值逐步向0偏向，同样会导致梯度消失问题。于是He Kaiming等人研究出了MSRA初始化法，又叫做He初始化法。<font color=green>从这张表格，就是可以看出，这个ReLU激活函数的数值，随着网络深度的增加，都是逐渐往0靠近了。对于梯度来说，就是消失了（因为0附近的梯度是0嘛）。所以，这里我也是明白了梯度消失的含义了，就是梯度没有了（这里也是回答了section 5.0梯度消失的含义了）。</font>

### 15.1.4 MSRA初始化方法

MSRA初始化方法$^{[2]}$，又叫做He方法，因为作者姓何。

条件：正向传播时，状态值的方差保持不变；反向传播时，关于激活值的梯度的方差保持不变。<font color=red>ToDo: 这里我就是犯迷糊了，这个状态值，还有激活值到底是一个什么意思呢？因为前面的Xavier里面也是出现了类似的陈述？</font>

网络初始化是一件很重要的事情。但是，传统的固定方差的高斯分布初始化，在网络变深的时候使得模型很难收敛。VGG团队是这样处理初始化的问题的：他们首先训练了一个8层的网络，然后用这个网络再去初始化更深的网络。

“Xavier”是一种相对不错的初始化方法，但是，Xavier推导的时候假设激活函数在零点附近是线性的，显然我们目前常用的ReLU和PReLU并不满足这一条件。所以MSRA初始化主要是想解决使用ReLU激活函数后，方差会发生变化，因此初始化权重的方法也应该变化。<font color=green>虽然没有看原始的文献，但是这一句话还是非常的棒的，就是在Xavier里面，假设了零点附近时线性的，这个也是符合Sigmoid激活函数的特性。但是在ReLU里面，这个假设就是不存在的了。所以就是需要修改初始化方法。</font><font color=red>因为零点附近的线性假设不成立，-> 激活值的方差改变？（不理解）-> 初始化方法就是需要改变（理解）</font>


只考虑输入个数时，MSRA初始化是一个均值为0，方差为2/n的高斯分布，适合于ReLU激活函数：

$$
W \sim N 
\begin{pmatrix} 
0, \sqrt{\frac{2}{n}} 
\end{pmatrix}
$$

$$
W \sim U 
\begin{pmatrix} 
-\sqrt{\frac{6}{n_{in}}}, \sqrt{\frac{6}{n_{out}}} 
\end{pmatrix}
$$

图15-4中的激活值从0到1的分布，在各层都非常均匀，不会由于层的加深而梯度消失，所以，在使用ReLU时，推荐使用MSRA法初始化。

<img src="../Images/15/init_msra_relu.png" ch="500" />

图15-4 MSRA初始化在ReLU激活函数上的表现

对于Leaky ReLU：

$$
W \sim N \begin{bmatrix} 0, \sqrt{\frac{2}{(1+\alpha^2) \hat n_i}} \end{bmatrix}
\\\\ \hat n_i = h_i \cdot w_i \cdot d_i
\\\\ h_i: 卷积核高度，w_i: 卷积核宽度，d_i: 卷积核个数
$$

### 15.1.5 小结

表15-2 几种初始化方法的应用场景

|ID|网络深度|初始化方法|激活函数|说明|
|---|---|---|---|---|
|1|单层|零初始化|无|可以|
|2|双层|零初始化|Sigmoid|错误，不能进行正确的反向传播|
|3|双层|随机初始化 <font color=green>随机初始化应该就是标准初始化</font>|Sigmoid|可以|
|4|多层|随机初始化|Sigmoid|激活值分布成凹形，不利于反向传播|
|5|多层|Xavier初始化|Tanh|正确|
|6|多层|Xavier初始化|ReLU|激活值分布偏向0，不利于反向传播|
|7|多层|MSRA初始化|ReLU|正确|

从表15-2可以看到，由于网络深度和激活函数的变化，使得人们不断地研究新的初始化方法来适应，最终得到1、3、5、7这几种组合。<font color=green>非常的棒，这里1、3、5、7的多种组合的原因，我能够大体上看明白，虽然还是有一些细节性的地方我还是不太明白的。</font> <font color=blue> 我的理解，这里的多层网络， 也是可以是深度神经网络了</font>

### 代码位置

ch15, Level1

### 思考与练习

1. 多层时，不能用零初始化。但是如果权重矩阵的所有值都初始化为0.1，是否可以呢？<font color=green>这个也是不行的，类似于0初始话，所有的每一层的输出值是一样的，那么BP的数值也是一样的，所以就是会导致最终的权重都是一样的。 </font>
2. 用14.6中的例子比较Xavier和MSRA初始化的训练效果。

### 参考资料

[1] Understanding the difficulty of training deep feedforward neural networks. link: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf

by Xavier Glorot, Yoshua Bengio in AISTATS 2010.

[这是中译版](https://blog.csdn.net/victoriaw/article/details/73000632)，感谢译者。

[2] 何凯明，Microsoft Research Asia，2015。https://arxiv.org/pdf/1502.01852.pdf

<font color=red> 这个section的内容很好，给出了各种不同的初始化方法&激活函数的组合。整个section的内容我也是理解了，除了一小部分之外，比如Xavier&MSRA中提到的条件，我不太明状态值的含义，我倒是明白了激活值的含义（激活函数的输出值）。另外，这个条件一味着什么呢？就是需要在训练过程中达到这样子的目的嘛？</font>

<font color=red> 这里[链接](https://zhuanlan.zhihu.com/p/25110150)的内容不错，也是讲权重矩阵初始化的。</font>


<font color=green> 代码"WeightsBias_2_0.py"给出了这个Section 5.1中各种不同的初始化方法了。 这里就是发生了一个非常奇怪的现象，就是Section 5.1公式和这里的代码突然是不匹配的了。具体点说，在代码部分，xavier使用的是uniform，而masa使用的是normal；但是在Section 5.1里面，uniform & normal的分布在两种不同的初始化方案里面都是出现。
    
但是后来查阅资料之后，就是发现了，Section 5.1给出的公式是对的，然后代码的公式也是正确的，可能就是Section 5.1中，多给了一些的公式了。 

xavier --> uniform, masa --> normal
</font>

